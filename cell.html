<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Praterich Cell</title>
<style>
:root {
  --bg: #0d1117;
  --primary: #00e0b8;
  --secondary: #444;
  --text: #fff;
}
body {
  margin: 0;
  font-family: "Inter", system-ui, sans-serif;
  background: var(--bg);
  color: var(--text);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100vh;
  overflow: hidden;
}
#orb {
  width: 140px;
  height: 140px;
  border-radius: 50%;
  background: radial-gradient(circle at 30% 30%, #00e0b8, #0066ff);
  box-shadow: 0 0 40px rgba(0, 224, 184, 0.6);
  transition: transform 0.3s ease, box-shadow 0.3s ease;
  animation: idlePulse 4s ease-in-out infinite;
}
@keyframes idlePulse {
  0%, 100% { transform: scale(1); box-shadow: 0 0 40px rgba(0,224,184,0.4);}
  50% { transform: scale(1.05); box-shadow: 0 0 60px rgba(0,224,184,0.8);}
}
#orb.listening {
  animation: listeningPulse 1s ease-in-out infinite;
}
@keyframes listeningPulse {
  0%,100% { transform: scale(1); box-shadow: 0 0 50px rgba(0,224,184,0.4);}
  50% { transform: scale(1.25); box-shadow: 0 0 80px rgba(0,224,184,1);}
}
#orb.speaking {
  animation: speakingPulse 0.6s ease-in-out infinite;
}
@keyframes speakingPulse {
  0%,100% { transform: scale(1); box-shadow: 0 0 60px rgba(0,224,184,0.5);}
  50% { transform: scale(1.15); box-shadow: 0 0 90px rgba(0,224,184,0.9);}
}
#text-display {
  margin-top: 40px;
  text-align: center;
  max-width: 80%;
  font-size: 1.1rem;
  color: #ccc;
  min-height: 4rem;
}
.controls {
  display: flex;
  gap: 12px;
  margin-top: 40px;
}
button {
  background: none;
  border: 2px solid var(--primary);
  color: var(--primary);
  border-radius: 50px;
  padding: 10px 28px;
  font-size: 1rem;
  cursor: pointer;
  transition: all 0.3s;
}
button:hover {
  background: var(--primary);
  color: var(--bg);
}
#camera-view {
  position: fixed;
  bottom: 20px;
  right: 20px;
  width: 180px;
  height: 120px;
  background: #000;
  border-radius: 10px;
  overflow: hidden;
  border: 2px solid var(--primary);
  opacity: 0;
  pointer-events: none;
  transform: scale(0.95);
  transition: opacity 0.3s, transform 0.3s;
}
#camera-view.active {
  opacity: 1;
  pointer-events: all;
  transform: scale(1);
}
#camera-view video {
  width: 100%;
  height: 100%;
  object-fit: cover;
}
</style>
</head>
<body>
  <div id="orb"></div>
  <div id="text-display">Hey there ðŸ‘‹ Iâ€™m Praterich Cell â€” tap below to talk.</div>

  <div class="controls">
    <button id="talk-btn">ðŸŽ¤ Talk</button>
    <button id="camera-btn">ðŸ“· Camera</button>
  </div>

  <div id="camera-view">
    <video id="user-video" autoplay playsinline></video>
  </div>

<script>
const API_URL = "https://praterich.vercel.app/api/praterich";
const orb = document.getElementById("orb");
const textDisplay = document.getElementById("text-display");
const talkBtn = document.getElementById("talk-btn");
const cameraBtn = document.getElementById("camera-btn");
const cameraView = document.getElementById("camera-view");
const userVideo = document.getElementById("user-video");

let recognizing = false;
let synth = window.speechSynthesis;
let recognition;
let cameraOn = false;
let stream;

// Speech recognition setup
if ('webkitSpeechRecognition' in window) {
  recognition = new webkitSpeechRecognition();
} else if ('SpeechRecognition' in window) {
  recognition = new SpeechRecognition();
}

if (recognition) {
  recognition.lang = "en-US";
  recognition.continuous = false;
  recognition.interimResults = false;

  recognition.onstart = () => {
    recognizing = true;
    orb.classList.add("listening");
    textDisplay.textContent = "ðŸŽ§ Listening...";
  };

  recognition.onresult = async (event) => {
    const userText = event.results[0][0].transcript.trim();
    orb.classList.remove("listening");
    textDisplay.textContent = `ðŸ—£ï¸ You: ${userText}`;
    await handleUserSpeech(userText);
  };

  recognition.onerror = (event) => {
    console.error(event.error);
    orb.classList.remove("listening");
    textDisplay.textContent = "âŒ I didnâ€™t catch that. Try again.";
    recognizing = false;
  };

  recognition.onend = () => {
    orb.classList.remove("listening");
    recognizing = false;
  };
}

async function handleUserSpeech(userText) {
  const body = {
    contents: [{ role: "user", parts: [{ text: userText }] }],
    system_instruction: {
      parts: [{ text: `You are Praterich, an AI by Stenoip Company. Speak conversationally, intelligently, and naturally.` }]
    }
  };

  try {
    orb.classList.add("speaking");
    textDisplay.textContent = "ðŸ’­ Thinking...";
    const res = await fetch(API_URL, {
      method: "POST",
      headers: {"Content-Type": "application/json"},
      body: JSON.stringify(body)
    });
    if (!res.ok) throw new Error("API error " + res.status);
    const data = await res.json();
    const aiText = data.text || "I couldnâ€™t get a response.";
    speak(aiText);
    textDisplay.textContent = `ðŸ¤– ${aiText}`;
  } catch (err) {
    console.error(err);
    textDisplay.textContent = "âš ï¸ Network or API error.";
  } finally {
    orb.classList.remove("speaking");
  }
}

function speak(text) {
  if (!synth) return;
  const utter = new SpeechSynthesisUtterance(text);
  utter.lang = "en-US";
  utter.rate = 1.1;
  utter.pitch = 1.0;
  synth.speak(utter);
}

talkBtn.addEventListener("click", () => {
  if (!recognition) {
    alert("Speech recognition not supported in this browser.");
    return;
  }
  if (recognizing) {
    recognition.stop();
    recognizing = false;
    orb.classList.remove("listening");
    textDisplay.textContent = "Stopped listening.";
  } else {
    synth.cancel();
    recognition.start();
  }
});

// Camera toggle
cameraBtn.addEventListener("click", async () => {
  if (!cameraOn) {
    try {
      stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
      userVideo.srcObject = stream;
      cameraView.classList.add("active");
      cameraBtn.textContent = "ðŸ“´ Stop Camera";
      cameraOn = true;
    } catch (err) {
      alert("Unable to access camera: " + err.message);
    }
  } else {
    if (stream) {
      stream.getTracks().forEach(track => track.stop());
    }
    cameraView.classList.remove("active");
    cameraBtn.textContent = "ðŸ“· Camera";
    cameraOn = false;
  }
});
</script>
</body>
</html>
